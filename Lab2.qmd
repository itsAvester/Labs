---
title: "Lab2"
author: "STAT 4610/5610 Spring 2026"
format: pdf
editor: visual
date: 2026-02-20
---

## Lab2: Comparing Nonlinear Regressions


```{r}
install.packages("ggplot2")
library(ggplot2)

install.packages("splines")
library(splines)

install.packages("ISLR2")
library(ISLR2)

install.packages("fANCOVA")
library(fANCOVA)

install.packages("boot")
library(boot)

install.packages("olsrr")
library(olsrr)

install.packages("caret")
library(caret)

```


**Due: 11:59PM Friday, February 27 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually fit **six** nonlinear regression models on a single X variable from your team's chosen dataset. You will use CV (including the built-in LOOCV functions in R) on at least three of these models to determine the best tuning parameters (e.g., $\lambda$ , K number of knots or steps, or degrees of freedom). You will compare the six models, choose the best one, make a prediction at an important value of X, interpret the results, and comment on the ethics of the situation.
2.  Teams will use the individual contributions to build a Generalized Additive Model (GAM), fit it, make predictions, make interpretations, show visualizations, etc.
3.  Each individual will comment on how they contributed to their team's GAM.

### Instructions for Lab2

In HW2, you applied several types of shrinkage and selection methods for linear models (shrinkage and selection methods) on a dataset of your choosing. This lab will be similar, but you will apply the nonlinear regression methods of Chapter 7 in *ISLR2* and use cross validation to choose the tuning parameters and select the best method. Individuals will model individual variables from the same dataset (one variable for each team member) and then combine their work into one team GAM that uses multiple X variables to predict Y.

#### What teams need to do first

1.  Choose a dataset that has enough interesting X variables that you can combine to model/predict Y. You can use one of the datasets that a teammate used for HW2 or pick a new one. You cannot use the Wage data from *ISLR2*.
2.  Decide who will model which X variable individually and when they need to be done to contribute to the team GAM.


```{r}
data(Hitters)

pairs(Hitters)

#remove missing values
Hitters <- na.omit(Hitters)
```

Our Y value will be Salary.

Corva will do CRBI

#### What individuals need to do

1.  Given your team's dataset and your X variable, fit a polynomial regression, step function regression, cubic spline, natural spline, smoothing spline, and a local regression (six models).
2.  Use CV to determine the optimal tuning parameters for at least three of your models. Sometimes just choosing the tuning parameter (e.g., picking the number of cuts in X for a step function regression) based on your knowledge of Q1 and Q3 makes more sense than using CV.
3.  Compare the CV MSE or other appropriate model summary (e.g., misclassification rate) for your models. Choose the best model method.
4.  At an important or significant value of X (explain why that value of x0 is important or significant), use your model to predict Y at that value.
5.  Interpret your prediction and generally what the model is telling you about Y.
6.  Comment on any ethical implications of this work.
7.  Optional: Create visualizations of your models or just of the best model.

## Corva Individual Section

I will do CRBI
```{r}
#plotting CRBI against Salary
ggplot(Hitters,
       aes(x=CRBI
           ,y=Salary))+
  geom_point()
```

```{r}
# polynomial regression

cpoly1 <- glm(Salary~CRBI , data = Hitters)
cpoly2 <- glm(Salary~poly(CRBI , 2), data = Hitters)
cpoly3 <- glm(Salary~poly(CRBI , 3), data = Hitters)
cpoly4 <- glm(Salary~poly(CRBI , 4), data = Hitters)
cpoly5 <- glm(Salary~poly(CRBI , 5), data = Hitters)
anova(cpoly1,cpoly2,cpoly3,cpoly4,cpoly5)


#here I tuned the parameter of degrees of my polynomial using anova. I will do it again using cv 

cv.poly.corva.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(Salary ~ poly(CRBI , i), data = Hitters)
  cv.poly.corva.error[i] <- cv.glm(Hitters , glm.fit)$delta[1]
}
cv.poly.corva.error


corvapolyfit <- glm(Salary~poly(CRBI,2), data=Hitters)

cpolyerror <- cv.glm(Hitters, corvapolyfit)$delta[1]
#126064
```

In the ANOVA output, the p-value comparing Model 1 and Model 2 is close to zero. Adding the quadratic term is significant. However, the p-value comparing Model 2 and Model 3 is around 0.367, indicating that adding a cubic term would not be significant. My cross-validated error has a sharp drop in the test MSE between the linear and the quadratic term but then goes back up again with a cubic term. Both of these suggest that I choose a quadratic for my polynomial regression.


```{r}
#step function regression

# table(cut(Hitters$Salary,4))
#the cut() function chooses our step to be at 666, 1260, and 1860 thousand if we cut the data into 4 but also those look pretty unbalanced so let's do quantiles instead

salaryhittersquantile <- quantile(Hitters$CRBI, probs = seq(0, 1, 0.25))

Hitters.Salary.Quantiles <- Hitters 
Hitters.Salary.Quantiles$Bins <- cut(
  Hitters.Salary.Quantiles$CRBI,
  breaks=salaryhittersquantile,
  include.lowest=TRUE
)

corvastepfit <- glm(Salary~Bins, data=Hitters.Salary.Quantiles)
csteperror <- cv.glm(Hitters.Salary.Quantiles, corvastepfit, K=10)$delta[1]
# 129688
```

```{r}
#cubic spline

#can't use cv.glm so we will use caret package to make the model and find cv error
#we will look at quantiles of the CRBI for our knots

traincontrol <- trainControl(method = "cv", number = 10) # this means we will do 10-fold CV
corvacubicsplinefit <- train(Salary ~ bs(CRBI, df = 4), 
                               data = Hitters, 
                               method = "lm", 
                               trControl = traincontrol) #df=4 means we have knots at the 25, 50, and 75th percentiles of CRBI
ccubicerror <- corvacubicsplinefit$results$RMSE^2 
#129651.1

```

```{r}
#natural spline

traincontrol <- trainControl(method = "cv", number = 10) # this means we will do 10-fold CV
corvanaturalsplinefit <- train(Salary ~ ns(CRBI, df = 4), 
                               data = Hitters, 
                               method = "lm", 
                               trControl = traincontrol) #df=4 means we have knots at the 25, 50, and 75th percentiles of CRBI
cnaturalerror <- corvanaturalsplinefit$results$RMSE^2 
#114827.4

```

```{r}
#smoothing spline

corvasmoothingsplinefit <- smooth.spline(Hitters$CRBI , Hitters$Salary , cv = FALSE)


#here, cv=FALSE because there are non-unique CRBI values. This means we don't do LOOCV but instead do cross-validation with Generalized cross-validation to select the lambda. cross-validated lambda is around 31

#Here, I tuned my lambda parameter, It is built in to the smooth.spline() functon

csmootherror <- corvasmoothingsplinefit$cv.crit # this is GCV not CV because we have some repeat x values
# 113303
```

```{r}
#local regression

#corvalocalfit <- loess(Salary ~ CRBI , span = .2, data = Hitters)
#corvalocalfit2 <- loess(Salary~CRBI , span = .5, data = Hitters)

#above is how I would do it if i had a span of 20% or 50% of the data - from ISLR2

#trying to tune this parameter of span - don't want to do this by hand so using package fANCOVA
#this will find the optimal span for the lowest gcv
corvalocalfit <- loess.as(x=Hitters$CRBI,
                          y=Hitters$Salary,
                          criterion="gcv") #regular cv won't work bc duplicate x vals

summary(corvalocalfit) #span of 0.2283372

#unfortunately fANCOVA uses GCV to find the best span but doesn't keep the actual GCV value so must be calculated manually. Use the formula (7.52) in ESL

str(corvalocalfit)
localN <- 263 #we have 263 players
localnumerator <- sum(corvalocalfit$residuals^2)/localN
localtraceS <- corvalocalfit$enp #this is "effective number of parameters"

clocalerror <- localnumerator/(1-localtraceS/localN)^2
#1164654.4
```

```{r}
corvaresults <- c(cpolyerror,csteperror,ccubicerror,cnaturalerror,csmootherror,clocalerror)
corvaresults
```

It appears that the best model for CRBI, based on cross-validated error, is the smoothing spline.

A significant number of career RBI is 1000. Generally, based on my knowledge of the game, 100 RBIs per season marks an excellent player/team, and around 1000 career RBIs would be indicative of a great career player. 

```{r}
#making prediction

corvapredsmooth <- predict(corvasmoothingsplinefit, x=1000)
corvapredsmooth

median(Hitters$Salary)
```

It appears that, for a player with 1,000 career RBIs, their predicted salary would be 1455 thousand dollars. The median salary for all players in the Hitters dataset is 425 thousand, so 1455 thousand dollars is a very high salary. This makes sense - players with such a high amount of career RBIs would be paid more.

CRBI might not be the best predictor of Salary if major league baseball executives were trying to figure out what salary to pay players based on their playing statistics. Excellent players on terrible teams will have lower CRBI values than if they were on excellent teams because it is more likely to get an RBI if your teammates are already on base. In order to get a clearer view of what to pay their players, major league baseball executives should take into account all statistics and situations of each individual player.

## Parker Individual Section

```{r}
#Polynomial Regression
set.seed(123)

poly_reg <- glm(Salary ~ poly(Hits, 2), data=Hitters2, family="gaussian")
x_grid <- seq(min(Hitters2$Hits),
              max(Hitters2$Hits),
              length.out = 200)
poly_pred <- predict(poly_reg, newdata = data.frame(Hits = x_grid))
plot(Hitters2$Hits, Hitters2$Salary)
lines(x_grid, poly_pred, lwd=2, col="red")

#Five Fold Cross Validation for Polynomial Regression

poly_reg_cv <- cv.glm(data=Hitters2, glmfit=poly_reg, K=5)

poly_reg_cv$delta[1]
```
```{r}
#Step Function Regression

#Cross Validation to Find the Optimal Number of Cuts
set.seed(123)
cuts <- 1:10
hits_range <- range(Hitters2$Hits)
step_results <- rep(NA,9)

for (k in 2:10){
  breaks <- seq(hits_range[1], hits_range[2], length.out = k + 1)
  
  step_model <- glm(Salary ~ cut(Hits, breaks = breaks, include.lowest = TRUE), data=Hitters2)
  cv_cut <- cv.glm(data=Hitters2, glmfit=step_model, K=5)
  
  step_results[k]=cv_cut$delta[1]
}

best_cut <- which.min(step_results)

#Step Regression With Optimal Number of Cuts
  
x_grid <- seq(min(Hitters2$Hits),
              max(Hitters2$Hits),
              length.out = 200)

step_reg <- glm(Salary ~ cut(Hits, 4), data=Hitters2)
step_pred <- predict(step_reg, newdata=data.frame(Hits = x_grid))
plot(Hitters2$Hits, Hitters2$Salary)
lines(x_grid, step_pred, lwd=2, col="red")

#Five Fold Cross Validation Step Function

step_results[4]
```
```{r}
#Cubic Spline

#Find the Optimal Number of Knots for Cubic Spline
set.seed(123)

cubic_results <- rep(NA,10)
hit_limits <- range(Hitters2$Hits)

for (k in 1:10){
  probs <- seq(0, 1, length.out = k + 2)[-c(1, k + 2)]
  knots <- quantile(Hitters2$Hits,probs)
  
  cubic_model <- glm(Salary~bs(Hits, knots=knots, Boundary.knots=hit_limits), data=Hitters2, family="gaussian")
  cubic_cv <- cv.glm(data=Hitters2, glmfit=cubic_model, K=5)
  cubic_results[k] <- cubic_cv$delta[1]
}

probs_optimal <- seq(0,1,length.out=5)[-c(1,5)]

cubic_spline <- glm(Salary ~ bs(Hits, knots=probs_optimal), data=Hitters2, family="gaussian")
cubic_spline_pred <- predict(cubic_spline, newdata=data.frame(Hits = x_grid))

plot(Hitters2$Hits, Hitters2$Salary)
lines(x_grid, cubic_spline_pred, lwd=2, col="red")

#Five Fold Cross Validation Cubic Spline With Optimal Number of Knots

cubic_results[3]
```
```{r}
#Natural Spline 
set.seed(123)

natural_spline <- glm(Salary~ns(Hits, df=3), data=Hitters2, family="gaussian")
natural_pred <- predict(natural_spline, newdata=data.frame(Hits = x_grid))

plot(Hitters2$Hits, Hitters2$Salary)
lines(x_grid, natural_pred, lwd=2, col="red")

#Cross Validation with Natural Spline

natural_cv <- cv.glm(data=Hitters2,glmfit=natural_spline, K=5)
natural_cv$delta[1]
```
```{r}
#Smoothing Spline

#Find the Optimal Tuning Parameter
smooth_spline <- smooth.spline(Hitters2$Hits, Hitters2$Salary, cv=TRUE)
best_df <- smooth_spline$df

smooth_spline_optimal <- smooth.spline(Hitters2$Hits, Hitters2$Salary, df=best_df)
smooth_pred <- predict(smooth_spline_optimal, x=x_grid,se=TRUE)

plot(Hitters2$Hits, Hitters2$Salary)
lines(x_grid, smooth_pred$y, lwd=2, col="red")

#Cross Validated Error Smooth Spline

smooth_cv <- smooth_spline$cv.crit
smooth_cv
```
```{r}
#Local Regression

local_reg <- loess(Salary~Hits, span=.4, data=Hitters2)
local_pred <- predict(local_reg, newdata=data.frame(Hits = x_grid))

plot(Hitters2$Hits,Hitters2$Salary)
lines(x_grid, local_pred, lwd=2, col="red")


#Cross Validation For Local Regression With Span=0.4
set.seed(123)
K<-5
span=0.4
folds <- sample(rep(1:5, length.out=nrow(Hitters2)))

mse <- function(actual, predicted){
  mean((actual-predicted)^2)
}

mse_vals <- numeric(5)

for (i in 1:5){
  training_data <- Hitters2[folds!=i,]
  testing_data <- Hitters2[folds==i,]
  
  loess_model <- loess(Salary~Hits, span=0.4, data=training_data)
  predictions <- predict(loess_model, newdata=testing_data)
  
  valid_idx <- !is.na(predictions)
  
  mse_vals[i] <- mse(testing_data$Salary[valid_idx],predictions[valid_idx])
}

mean(mse_vals)
```
Comparing the six models, the best model in terms of MSE is the local regression model with a span of 0.4. This model had an MSE of 152988. The worst models, although not by too much, were the polynomial regression and the cubic spline. I chose the local regression model because it had the lowest cross validated error. 

```{r}
#Using Local Regression Model for Prediction
local_reg <- loess(Salary~Hits, span=0.4, data=Hitters2)

warm_bern <- data.frame(Hits=35)
warm_bern_pred <- predict(local_reg, newdata=warm_bern)
warm_bern_pred
```

I'll pretend that I am some executive at the Colorado Rockies, and we are trying to come up with reasonable salary propositions for players who are at the end of their contract. We are are specifically looking at Warming Bernabel, who had 35 hits in the 2025 season for the Rockies, Using our model, a reasonable salary is 325,689 dollars a year for however long the contract is.

There are not many ethical implications of the model because the data is publicly available; the model does not intake sensitive information that we need to worry about being leaked. The local regression model should also be easily interpretable visually, so there is not much of a worry for the misuse or misinterpretation of the model among stakeholders. 




#### What teams need to do

1.  Create a GAM based on the work of your individual team members.
2.  Visualize the GAM (R has some good built-in plotting functions, such as plot.gam()).
3.  Choose a value of X that is important or interesting (explain why) and use that point to make a prediction for Y. Interpret this prediction.
4.  How does the GAM compare with a linear method (e.g., lasso, ridge, PCR, etc.)?


```{r}
# linear method Lasso

set.seed(123)
rowsHitters <- sample(x=nrow(Hitters), size=floor(0.75*nrow(Hitters2)), replace=FALSE)
train <- Hitters[rowsHitters,]
test <- Hitters[-rowsHitters,]

x_train <- model.matrix(Salary ~ ., data = train)[, -1]
y_train <- train$Salary

lassohittermodel <- cv.glmnet(x=x_train,y=y_train,family="gaussian",
                      standardize=TRUE,alpha=1, nfolds=10, type.measure="mse")
lambda.lasso <- lassohittermodel$lambda.min

x_test <- model.matrix(Salary ~ ., data = test)[, -1]
y_test <- test$Salary
lassopredict <- predict(lassohittermodel, s=lambda.lasso, newx=x_test)
lasso_mse <- mean((lassopredict - test$Salary)^2)
lasso_mse
```
The lambda parameter for the Lasso method was selected using cross-validation. The MSE from the lasso was around 1106.

```{r}
#Ridge Regression Model for Comparison
library(glmnet)

Hitters2 <- na.omit(Hitters)

set.seed(123)

ridge_rows <- sample(x=nrow(Hitters2), size=floor(0.75*nrow(Hitters2)), replace=FALSE)
ridge_train <- Hitters2[ridge_rows,]
ridge_test <- Hitters2[-ridge_rows,]

x_ridge_train <- model.matrix(Salary~., data=ridge_train)[,-1]
y_ridge_train <- ridge_train$Salary

ridge_model <- cv.glmnet(x=x_ridge_train, y=y_ridge_train, family="gaussian", nfolds=10,
                         standardize=TRUE, alpha=0,type.measure="mse")
ridge_lambda <- ridge_model$lambda.min

x_ridge_test <- model.matrix(Salary~., data=ridge_test)[,-1]
y_ridge_test <- ridge_test$Salary

ridge_pred <- predict(ridge_model, s=ridge_lambda, newx=x_ridge_test)
ridge_mse <- mean((ridge_pred-y_ridge_test)^2)
ridge_mse
```
The ridge model that we will compare to our GAM to had an mse of 127917.5

Insert analysis here about how the lasso compared to our GAM, when we have completed it. 


#### Individual contributions to team GAM

Each individual must comment on their contributions to the team GAM. For example, write something like: ‘I told person X that my results showed that the natural spline with 4 knots was the best model for variable x1. Then I created a visualization to help interpret the GAM.’ Or, ‘I used the GAM to make a prediction for the point x0.’ Or, ‘I didn’t actually do anything for the team section.’ Only individuals who contribute to the team section will get points for the team section.

Corva: I did lasso model selection so that we could compare it to our GAM.
Parker: Did Ridge model to compare to GAM

#### **Some intended outcomes from this assignment:**

-   You will individually get practice fitting a nonlinear regression models: polynomial regression, step function regression, cubic spline, natural spline, smoothing spline, and local regression
-   You will practice using CV to select the optimal tuning parameters of your models
-   You will use CV to compare the performance of these models
-   You will make predictions based on your models
-   You will practice fitting and interpreting a GAM
-   You will gain experience working on a data science "project" as a team
