---
title: "Lab1"
author: "Eric Vance, Alice Warnick, Parker Breaux, Averey Copeland, Corva Graham"
format: pdf
editor: visual
date: 2025-02-02
---

## Lab1: Comparing Classification Methods

**Due: 11:59PM Friday, February 7 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually fit five classification models and compare their sensitivities and specificities.
2.  You will interpret these models and make a prediction in your individual section.
3.  As a team, you will create one visualization that summarizes the performance of the models.

### Instructions for Lab1

In Lab0, the professor created a generating model for Y variables taking on 0 or 1 values based on the X1 and X2 inputs. You came up with the backstory for what Y, X1, and X2 were and why it was important to use X1 and X2 to predict Y.

In this lab, you will apply four newly learned statistical learning methods to continue your analyses for how to best predict or explain Y from X1 and X2. You should continue to use the Q1 qualitative context you developed in Lab0 (or you can develop a new one if you want).

Each individual will fit five classification models on a training dataset and then evaluate how well those models classify Y based on a test dataset. Individuals will make a prediction given (x1, x2) and then interpret their prediction and make a recommendation for action. Just as in Lab0, each individual will describe the Q1, Q2, and Q3 for this "project". Specifically, for Q1: What is Y, X1, and X2, and why should we care?. Train your five models on the training set, compare the sensitivity and specificity of the models on the test set, make a prediction given (x1, x2) (this is Q2). Then describe what actions (Q3) you recommend given your Q1 context and your Q2 results. Reflect on some ethical aspect of this project.

#### Generating Model

We have a logistic regression generating model. Given $x_1 \in [0,1]$ and $x_2 \in [0,1], Y \sim Ber(p)$, where $p$ is related to $x_1$ and $x_2$ through the logistic link function: $\log(\frac{p}{(1-p)}) = x_1 -2x_2 -2{x_1}^{2} + {x_2}^{2} + 3{x_1}{x_2} +4{x_1}{x_2}^2 -3{x_1}^{2}x_2$, where $\log$ is the natural log, sometimes written as $\ln$.

The code for this is below.

```{r}
library(class)
suppressPackageStartupMessages(library(tidyverse))
```

```{r}
#Generative model
set.seed(200) #setting a random seed so that we can
#reproduce everything exactly if we want to

generate_y <- function(x1,x2) { #two input parameters to generate the output y
  logit <- x1 -2*x2 -2*x1^2 + x2^2 + 3*x1*x2 +4*x1*x2^2 -3*x1^2*x2
  p <- exp(logit)/(1+exp(logit)) #apply the inverse logit function
  y <- rbinom(1,1,p) #y becomes a 0 (with prob 1-p) or a 1 with probability p
}
```

#### Training dataset

We are going to use our generating model to create a training dataset of 1000 predictors (x1, x2), and then 1000 outcomes. Then we plot all three variables to see what our training data looks like.

```{r}
# Generate a training dataset with 1000 points
set.seed(200)
n = 1000
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 1000 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #How many 0's and 1's were predicted? In this
#training set, almost 53% were 1's. When n is very large
# about 51.5% of the Y's are 1's. That's really close to
# 50/50 so we shouldn't have issues with "imbalance"
# which is something we'll learn about later in the 
# semester.

training <- cbind(X1,X2,Y) #combining all of my variables into a training dataset
ggplot(data=training, aes(x=X1, y=X2, color=Y)) +
  geom_point()
```

How well will various classifiers predict Y given new x1 and x2 values?

#### Test datasets

Each individual will generate a test set of 1000 predictors (x1, x2) and outcomes (y) that we will use as our "ground truth".

So, create your individual test dataset (using random seed=201, 202, 203, or 204; each teammate has a different test dataset).

```{r}
# Create the training dataset as above using seed=200
# Create a testing dataset using seed=201, 202, 203, or 204

set.seed(201)
n = 1000

```

#### What individuals need to do

1.  Given the training set (seed=200), fit:
    1.  logistic regression model
    2.  linear discriminant analysis (LDA)
    3.  quadratic discriminant analysis (QDA)
    4.  naive Bayes
    5.  KNN with k=optimal k from Lab0
2.  Calculate the sensitivity, specificity, and overall error rate (misclassification rate) for each model given your test set (your seed=201 or 202 or 203 or 204).
3.  Make a prediction for a new point (x1, x2) = (0.25, 0.25) or (0.25, 0.75) or (0.75, 0.25) or (0.75, 0.75) for each fitted model. Each individual will have a different point for their predictions.
4.  Summarize the Q1, Q2, and Q3 aspects of this "project."

### Corva Individual Section

```{r}

# creating testing set

set.seed(201)
X1 <- runif(1000,0,1)
X2 <- runif(1000,0,1)

Y <- rep(0,1000)
for (i in 1:1000) {
  Y[i] <- generate_y(X1[i],X2[i])
}

testingcorva <- cbind(X1, X2, Y)
testingcorva <- data.frame(testingcorva)

#my point is (0.25, 0.25)
corvapoint <- data.frame(X1=0.25, X2=0.25)
```

```{r}
#fit logistic regression model

lrcorvafit <- glm(Y~X1+X2, data=data.frame(training), family="binomial")
lrcorvaprob <- predict(lrcorvafit, newdata=testingcorva, type="response")
lrcorvaclass <- testingcorva %>%
  mutate(classprob = lrcorvaprob,
         predclass = ifelse(classprob > 0.5, 1, 0))

lrcorvamisclass <- mean(lrcorvaclass$Y != lrcorvaclass$predclass)

table(lrcorvaclass$Y, lrcorvaclass$predclass)

lrsensitivity <- 365/(365+166)
lrspecificty <- 226/(226+243)

lrcorvapointprob <- predict(lrcorvafit, newdata=corvapoint, type="response")
lrcorvapoint <- ifelse(lrcorvapointprob > 0.5, 1, 0)
```

```{r}
#fit LDA model

ldacorvafit <- lda(Y~X1+X2, data=data.frame(training))
ldacorvaprob <- predict(ldacorvafit, newdata=testingcorva, type="response")
ldacorvaclass <- ldacorvaprob$class

ldacorvamisclass <- mean(ldacorvaclass != testingcorva$Y)

table(testingcorva$Y, ldacorvaclass)

ldasensitivity <- 364/(364+167)
ldaspecificity <- 226/(226+243)

ldacorvapointprob <- predict(ldacorvafit, newdata=corvapoint)
ldacorvapoint <- ldacorvapointprob$class
```

```{r}
#fit QDA model

qdacorvafit <- qda(Y~X1+X2, data=data.frame(training))
qdacorvaprob <- predict(qdacorvafit, newdata=testingcorva, type="response")
qdacorvaclass <- qdacorvaprob$class

qdacorvamisclass <- mean(qdacorvaclass != testingcorva$Y)

table(testingcorva$Y, qdacorvaclass)

qdasensitivity <- 306/(306+225)
qdaspecificity <- 314/(314+155)

qdacorvapointprob <- predict(qdacorvafit, newdata=corvapoint)
qdacorvapoint <- qdacorvapointprob$class
```

```{r}
#fit naive Bayes
library(e1071)
nbcorvafit <- naiveBayes(Y~X1+X2, data = data.frame(training))
nbcorvaclass <- predict(nbcorvafit, newdata=testingcorva)

nbcorvamisclass <- mean(nbcorvaclass != testingcorva$Y)

table(testingcorva$Y, nbcorvaclass)

nbsensitivity <- 340/(340+191)
nbspecificity <- 284/(284+185)

nbcorvapoint <- predict(nbcorvafit, newdata=corvapoint)
```

```{r}
#fit KNN with k=5 which is the optimal k from Lab0

knncorvafit <- knn(train = training[,1:2],
                   test = testingcorva[,1:2],
                   cl=training[,3],
                   k=5)

knncorvamisclass = 1-(sum(knncorvafit==testingcorva[,3])/length(testingcorva$Y))

table(testingcorva$Y, knncorvafit)

knnsensitivity <- 314/(314+217)
knnspecificity <- 254/(254+215)

knncorvapoint <- knn(train=training[,1:2],
                     test=corvapoint[,1:2],
                     cl=training[,3],
                     k=5)
```


```{r}
# Table with all of the results
corvatab <- matrix(c(0.409,0.41,0.38,0.376,0.432,
                0.482,0.685,0.576,0.640,0.591,
                0.687,0.481,0.670,0.606,0.541,
                "1","1","1","1","1"),byrow=TRUE, nrow=4)
rownames(corvatab) <- c('Misclassification', 'Sensitivity', 'Specificity', 'Prediction')
colnames(corvatab) <- c('Logistic','LDA','QDA','Naive','KNN')
corvaresults <- data.frame(corvatab)
```

Q1: In this “project”, Y was whether or not a borrower defaulted on their loan (default=1,no default=0), given their age(X1) and income (X2). The specific prediction is for an age 25 borrower (X1=0.25) who earns $25,000 a year (X2=0.25). This project is important for banks to understand which borrowers are risky to give loans to. 

Q2:
```{r}
corvaresults
```

It appears that misclassification rates of the models are all around 40%. The lowest misclassification rate was with 37.6% with Naive Bayes and the highest misclassification rate was 43.2% with KNN. The model with the highest sensitivity was LDA with 68.5% and the lowest sensitivity was Logistic at 48.2%. The model with the highest specificity was Logistic at 68.7% and the model with the lowest specificity was LDA at 48.1%. Given a 25 year old borrower who earns $25,000 a year, all of the models predicted that the borrower would default.

Q3:
I would recommend not lending to this 25 year old borrower who earns $25,000 a year since all of the models used predicted that he would default on his loan. This loan modelling has ethical implications since it only considers predictors age and income. Other predictors like loan size could be overlooked, and this borrower could be denied a loan that they really needed but could reasonably pay off.






## Parker Individual Section

```{r}
# Create the training dataset as above using seed=200
# Create a testing dataset using seed=201, 202, 203, or 204

set.seed(202)
n = 1000
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

Y <- rep(0,n)
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

testing <- cbind(X1, X2, Y)
```

 ```{r}
    training <- as.data.frame(training)
    testing <- as.data.frame(testing)
    ```
 ```{r}
    #Logistic Regression Model
    logistic_model <- glm(Y~X1+X2, data=training, family="binomial")

    logistic_prob <- predict(logistic_model, newdata=testing, type="response")

    testing_logistic <- testing%>%
      mutate(logistic_prob = logistic_prob,
             logistic_pred = ifelse(logistic_prob > 0.5, 1, 0))

    #Misclassification Rate
    mean(testing_logistic$logistic_pred != testing_logistic$Y)

    #Confusion Matrix
    glm.probs<-predict(logistic_model,testing, type="response")
    glm.predicts<-factor(ifelse(glm.probs > 0.5,"1", "0"))
    Y2_factor <- as.factor(testing_logistic$Y)

    confusionMatrix(glm.predicts, Y2_factor,
                    positive="1")

    #Prediction for point (0.25, 0.75)
    point <- data.frame(X1=c(0.25), X2=c(0.75))

    logistic_point <- predict(logistic_model, point, type="response")
    logistic_point
    ```
```{r}
#Linear Discriminant Analysis

lda_model <- lda(Y~X1+X2, data=training)

lda_pred <- predict(lda_model, testing)

#Misclassification Rate
lda_predictions <- lda_pred$class

actual <- as.factor(testing$Y)

mean(lda_predictions != actual)

#Confusion Matrix
confusionMatrix(lda_predictions, actual, positive="1")

#Prediction for (0.25, 0.75)
lda_point <- predict(lda_model, point)
lda_point
```
```{r}
#Quadratic Discriminant Analysis

qda_model <- qda(Y~X1+X2, data=training)

qda_pred <- predict(qda_model, testing)

#Misclassification Rate
mean(qda_pred$class != actual)

#Confusion Matrix
confusionMatrix(qda_pred$class, actual, positive="1")

#Prediction for (0.25, 0.75)
qda_point <- predict(qda_model, point)
qda_point
```
```{r}
#Naive Bayes
library("e1071")

bayes_naive <- naiveBayes(Y~X1+X2, data=training)

naive_pred <- predict(bayes_naive, newdata=testing)

#Confusion Matrix
cm <- table(actual, naive_pred)

confusionMatrix(cm, positive="1")

#Prediction for (0.25, 0.75)

naive_point <- predict(bayes_naive, point)
naive_point
```
```{r}
#KNN with Optimal K
#Find Optimal k with seed 202
set.seed(202)
vals <- rep(0,50)

misclass_rate <- function(predictions, truth) {
  1-(sum(predictions == truth)/length(predictions))
}

for(i in 1:50){
 preds <- knn(train=training[,1:2], test=testing[,1:2], cl=training[,3], k=i) 
 vals[i] <- misclass_rate(preds, testing[[3]])
}
optimal_k <- which.min(vals)

knn_model <- knn(train=training[,1:2], test=testing[,1:2], cl=training[,3],
                 k=optimal_k)

#Mislclassification Rate
vals[47]

#Confusion Matrix

confusionMatrix(knn_model, actual, positive="1")

#Predicton for (0.25, 0.75)

knn_point <- knn(train = training[,1:2], test=point[,1:2], cl=training[,3], 
    k=optimal_k)
knn_point
```
```{r}
# Table with all of the results
tab <- matrix(c(0.412,0.413,0.38,0.392,0.381,
                0.657,0.655,0.549,0.612,0.571,
                0.519,0.519,0.691,0.604,0.665,
                1,1,1,1,1),byrow=TRUE, nrow=4)
rownames(tab) <- c('Misclass.', 'Sensitivity', 'Specificity', 'Prediction')
colnames(tab) <- c('Logistic','LDA','QDA','Naive','KNN')
results <- data.frame(tab)
results
```
## Q1, Q2, and Q3
Q1: I will stick with the story I had for lab zero, where I imagined creating a model for the purpose of fraud detection for financial institutions. I will imagine that X1 is the purchase amount, and then X2 will be the distance from the home address, where each observation is a purchase made with a credit card. We want to create a classification model, which could be a logistic, LDA, QDA, Naive Bayes, or KNN model. Some of these models are more simple than others (logistic, KNN), while the others may be harder to interpret, so we have to keep that in mind when we decide which model to use. A financial institution will also likely have a preference for a model with a higher true positive rate (sensitivity), rather than a higher true negative rate (specificity). This is because we want our model to correctly predict fraud as much as possible. 

Q3: Looking at the results, it looks like the models with the lowest misclassification rates were QDA and KNN. They both wrongly predicted whether a purchase was fraud about 38 percent of the time. However, the models with the highest sensitivity were logistic, LDA, and Naive Bayes, which all correctly predicted fraud about 60 percent of the time. At this step, we would also discuss the assumptions of these models, like how LDA and QDA assume the data is normally distributed, along with Naive Bayes that assumes the predictors are conditionally independent given the class. All things considered, the most enticing model could be the Naive Bayes model because it has high sensitivity and specificity, as well as having one of the lowest misclassification rates. One of the assumptions of Naive Bayes is that given the class, the predictors are independent. I think this can be assumed because if we are given that a purchase was fraudulent, whether a purchase is large or not does not give us any information about the location of the purchase. One drawback of Naive Bayes is that it would be harder to interpret to non-domain experts, so maybe we want to look at the logistic regression model. 

#### Averey Copeland Individual Section
# Create the training dataset as above using seed=200
# Create a testing dataset using 204
```{r}
library(naivebayes)
set.seed(200)
n = 1000


#Generative model
set.seed(204) 

n = 1000
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 1000 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

testingdata <- cbind(X1,X2,Y)

trainingdata <- as.data.frame(trainingdata)
testingdata <- as.data.frame(testingdata)
model1 <- glm(Y~X1+X2,data=trainingdata,family="binomial")
model2 <- lda(Y~X1+X2,data=trainingdata)
model3 <- qda(Y~X1+X2,data=trainingdata)
#Kept getting error: Y has to be a factor so as.factor(Y) fixes that
model4 <- naive_bayes(as.factor(Y)~X1+X2,data=trainingdata)
model5 <- knn(train = trainingdata[,1:2], test = testingdata[,1:2], cl=trainingdata[,3],k=8)

#logistic model
prob1 <- predict(model1, newdata = testingdata, type = 'response')
pred1 <- ifelse(prob1>.5, 1,0)

confusion_matrix <- table(True=testingdata$Y, Pred=pred1)
confusion_matrix
#Sensitivity = TP/(TP+FN)
Logisticsens <-  35/455
Logisticsens


#Specificity = TN/(TN+FP)
Logisticspec <-  532/550
Logisticspec


#Total Error
logisticerr <- (13+420)/(13+420+532+35)
logisticerr


#LDA model
prob2 <- predict(model2, newdata=testingdata, type = 'response')
class <- prob2$class
misclassification <- mean(class!=testingdata$Y)
confusion_matrix2 <- table(True=testingdata$Y, class)
confusion_matrix2

#Sensitivity
LDAsens <- 35/(35+420)
LDAsens

#Specificity
LDAspec <- 534/(534+11)
LDAspec

#Error Rate
LDAerr <- (11+420)/(35+534+11+420)
LDAerr


#QDA
prob3 <- predict(model3, newdata=testingdata, type = 'response')
class <- prob3$class
misclassification <- mean(class!=testingdata$Y)
confusion_matrix3 <- table(True=testingdata$Y, class)
confusion_matrix3

#Sensitivity
QDASense<- 89/(89+366)
QDASense

#Specificity
QDAspec <- 494/(494+51)
QDAspec

#Error Rate
QDAerr <- (51+366)/(51+366+89+494)
QDAerr

#NAIVE BAYES
prob4 <- predict(model4, newdata=testingdata)

misclassification <- mean(prob4!=testingdata$Y)
confusion_matrix4 <- table(True=testingdata$Y, prob4)
confusion_matrix4

#Sensitivity
NBSense<- 66/(66+389)
NBSense

#Specificity
NBspec <- 507/(507+38)
NBspec

#Error Rate
NBerr <- (38+389)/(507+66+38+389)
NBerr


#KNN

confusion_matrix5 <- table(testingdata$Y, model5)
confusion_matrix5

#Sensitivity
KNNSense<- 155/(155+300)
KNNSense

#Specificity
KNNspec <- 423/(423+122)
KNNspec

#Error Rate
KNNerr <- (122+300)/(122+300+423+155)
KNNerr

#Point prediction (.75, .25)
point <- data.frame(x1=c(.75), x2=c(.25))

logistic<-predict(model1, point)
LDA<-predict(model2, point)
QDA<-predict(model3, point)
NB<-predict(model4, point)
#logistic
#LDA
#QDA
#NB

#RESULTS
tab <- matrix(c(0.077,0.195,0.196,0.145,0.340,
                0.979,0.979,0.906,0.930,0.776,
                0.431,0.431,0.417,0.427,0.422,
                0,0,0,0,0),byrow=TRUE, nrow=4)
rownames(tab) <- c('Specificity.', 'Sensitivity', 'Error', 'Prediction')
colnames(tab) <- c('Logistic','LDA','QDA','ND','KNN')
results <- data.frame(tab)
results
```
Q1: Lets say that this model predicts whether or not a pitch in baseball is a strike or a ball after only traveling 10 feet. Where Y is whether the baseball ended up being a strike or not. X1 is the height of the pitch after traveling 10 feet, x2 is the distance from the left batters box horizontally (0 touching the left batters box). 

Q2: This is the summary of each model

Q3: Given the relatively high error across all models, this could morally lead to players misjudging where a pitch might end up causing a batters batting average to decrease instead of increase as if the model was connected to a buzzer, this could lead the batter to not swing at pitches that they normally would have. Given the point (.75,.25) the batter would choose to take this pitch and hope it is a ball.

## Alice Individual Section
```{r}
# Create the training dataset as above using seed=200
# Create a testing dataset using seed=201, 202, 203, or 204

set.seed(203)
n = 1000
x1 <- runif(n, 0, 1)
x2 <- runif(n, 0, 1)
y <- rep(0, n)
for (i in 1:n) {
  y[i] <- generate_y(x1[i], x2[i])
}

testing <- as.data.frame(cbind(x1, x2, y))
training <- as.data.frame(training)
# ggplot(data = testing, aes(x=x1, y=x2, color = y)) + 
#  geom_point()
```

#### What individuals need to do

1.  Given the training set (seed=200), fit:
    1.  logistic regression model
    2.  linear discriminant analysis (LDA)
    3.  quadratic discriminant analysis (QDA)
    4.  naive Bayes
    5.  KNN with k=optimal k from Lab0
```{r}
library(caret)
# logistic
m_log <- glm(y ~ x1 + x2, data = training, family = binomial)
m_log_probs <- predict(m_log, newdata = testing)
Test_m_log <- testing %>%
  mutate(m_log_probs = m_log_probs, m_log_preds = ifelse(m_log_probs > .5, 1, 0))
misC_log <- mean(Test_m_log$m_log_probs != Test_m_log$y)
log_test_probs <- predict(m_log, newdata = testing, type = "response")
log_test_preds <- factor(ifelse(log_test_probs > .5, 1, 0))
y2 <- as.factor(Test_m_log$y)
# log model output
confusionMatrix(log_test_preds, y2, positive = "1")
# log point 
pt <- data.frame(x1 = c(.75), x2 = c(.75))
log_pt <- predict(m_log, pt, type = "response")
log_pt
```

```{r}
# lda
levels <- c("0", "1")
comp <- factor(testing$y, levels = levels)
library(MASS)
m_lda <- lda(y ~ x1 + x2, data = training)
m_lda_probs <- predict(m_lda, testing)
m_lda_preds <- m_lda_probs$class
misC_lda <- mean(comp != m_lda_preds)
confusionMatrix(m_lda_preds, comp, positive = "1")
lda_pt <- predict(m_lda, pt)
lda_pt
```

```{r}
# qda 
m_qda <- qda(y~x1+x2, data = training)
m_qda_probs <- predict(m_qda, testing)
m_qda_preds <- m_qda_probs$class
misC_qda <- mean(comp != m_qda_preds)
confusionMatrix(m_qda_preds, comp, positive = "1")
qda_pt <- predict(m_qda, pt)
qda_pt
```

```{r}
# naiive bayes
library(e1071)
m_nb <- naiveBayes(y~x1+x2, data = training)
m_nb_probs <- predict(m_nb, newdata = testing)
misC_nb <- mean(m_nb_probs != comp)
nb_pt <- predict(m_nb, pt)
nb_pt
confusionMatrix(table(comp, m_nb_probs), positive = "1")
```
```{r}
library(class)
m_knn <- knn(train = training[,1:2], test = testing[,1:2], cl = training[,3], k = 5)
misC_knn <- 1 - sum(m_knn == testing[,3])/length(testing$y)
table(testing$y, m_knn)
knn_pt <- predict(m_knn, pt)
knn_pt
```

```{r}
Atab <- matrix(c(0.432,0.5796,0.5562, _,
                 0.401, .6056, .5924, _
                 0.4, .6076, .5924, _,
                0.389,0.6137,0.6083, _, 
                 .37, .5498, .7108, _, 
                "1","1","1","1","1"),byrow=TRUE, nrow=4)
rownames(Atab) <- c('Misclassification', 'Sensitivity', 'Specificity', 'Prediction')
colnames(Atab) <- c('KNN', 'LDA', 'Logistic', 'Naive Bayes', 'QDA')
resultsA <- data.frame(Atab)
```
Q1: Given the high variability of this data, this could be attempting to predict the default rate on subprime loans based on normalized numeric factors like income and debt to help lenders make more profitable decisions selecting future clients based on their financial status. 
Q2: All of the models tested on this seed had misclassification rates between .37 and .44. with QDA and KNN (k = 5) respectively. My point (.75, .75) could represent somebody with upper-percentile income but also with upper-percentile debt compared to other borrowers. Most of the models predicted default for this borrower. 
Q3: When extrapolating predictions like this to the real world, it should also be noted that the prediction accuracy in a real-world scenario likely decreases as one approaches the limits of the range and strays from the central values due to the increased variability in personal circumstance required of somebody to end up in that higher income, higher debt category like my borrower is (75th percentile in both income and debt). 

```{r}
# Model Performance Summarization

yes <- results <- data.frame(
  Model = c("Logistic", "LDA", "QDA", "Naive Bayes", "KNN"),
  Misclassification = c(0.412,0.413,0.38,0.392,0.381,0.409,0.41,0.38,0.376,0.432),
  Sensitivity = c(0.657,0.655,0.549,0.612,0.571,0.482,0.685,0.576,0.640,0.591),
  Specificity = c(0.519,0.519,0.691,0.604,0.665,0.687,0.481,0.670,0.606,0.541))

library(tidyr)
library(dplyr)
library(ggplot2)

yes_long <- yes %>%
  pivot_longer(
    cols = -Model,
    names_to = "Metric",
    values_to = "Value"
  )

ggplot(yes_long, aes(x = Value, y = Model, color=Metric)) +
  geom_point(size = 3) +
  facet_wrap(~Metric, scales = "free_x") +
  scale_color_manual(values = c(
    "Misclassification" = "red",
    "Sensitivity" = "blue",
    "Specificity" = "darkgreen"
  ))+
  theme_minimal()

```

Because of situations including predicting loan default or predicting fraud, the best model would have a higher sensitivity. Sensitivity is an important metric because the consequences for not predicting loan default or fraud are higher than wrongly flagging someone for default or fraud. It appears that LDA has the highest sensitivity which would be the best model in this situation. 
