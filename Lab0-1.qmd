---
title: "Lab0"
author: "Eric Vance"
format: pdf
editor: visual
---

## **Lab0:** About Your Team, About Individuals, and Team Collaboration

**Due: 11:59PM Sunday, January 25 on Canvas as a knitted pdf file of a Quarto document**

1.  You will determine your team's name and goals.
2.  You will add to the document your individual sections.
3.  You will practice collaborating on an applied statistical learning "project."

### Instructions for Lab0

1.  Using Quarto, you will complete the first team section "About Team Teamname." See further instructions below.
2.  Each team member will add their own individual section to the team Quarto document. How do you want to collaborate on your document this semester? Github? Google Colab? Something else? See instructions for individual sections below.
3.  As a team, you will complete an applied "project". There will be some individual components (so that everyone gets practice with implementing the stat learning methods) and team components.

### About The Crows

insert group photo here. put a space between the close bracket and the open parentheses

\![Caption for the picture.\] (/path/to/image.png)

Our team name is The Crows. Our main goal for this semester for this course is to build a solid foundation in both coding and statistical learning methods in order to be equipped to use them in real-life settings.

#### Individual Sections

Each individual must complete their own subsection, which must include:

-   a photo of yourself with a caption explaining the context
-   at least one (non-statistics) question you would like to know the answer to that could potentially be answered by applying statistical learning methods to data
-   what you would love to be doing six months after graduation and then five years after that (what would make you excited to be doing?)
-   what you hope your greatest career accomplishment will be
-   and given these hopes and goals, what you are hoping to learn/accomplish/do in this course.
-   You must also include something of your own choosing not described above. Anything. Be creative!

### Corva Individual Written Section

\![Caption for the picture.\] (/path/to/image.png)
This photo was taken in September by my friends while we were attending the Colorado Flute Association's Flute Fair at DU. I had no idea that the original Chipotle was in Denver and was super excited because I love a good Chipotle bowl.

I would love to know when and how much it will snow next in Colorado, since it feels like most of this winter has been dry, windy, and cold. Using variable like temperature, cloud cover, and weather in other parts of the country, I could build some binary classification model (will it snow or will it not snow)?. I could also use some sort of regression to predict how much it will snow.

Six months after graduation, I would love to be living in and exploring a new city. In five years, I'd love to have obtained my ACAS (associate of the Casualty Actuary Society) and have a fulfilling job working in that field. In five years, I would also be so excited to adopt a pair of dogs and go on lots of hikes with them.

Although my biggest career goal in general is to have stable, satisfying work and lots of work-life balance (I would love to work remotely!), I think a big accomplishment would be to gain more credentials and work as some sort of team lead. I'd love to be in charge of something like financial strategy for a company.

Given I'd like to work as an actuary, I hope to gain a solid foundation in statistical learnining methods in this course. I would also like to gain more confidence in R in applying these methods.

\![Caption for the picture.\] (/path/to/image.png)
This is a photo from the first time I ever saw cows up close this summer. Cows are my favorite animal. I was trying to take a photo of this cow and I got startled because it licked me!! After I got scared, the herd of cows got also scared and they all started mooing very loudly. 

### Parker Individual Written Section

I have been applying to a lot of internships, with no luck. So if I could somehow compile some data, it would be interesting to apply a statistical learning method in order to answer this question: am I likely to get an interview if I apply for this internship? I would probably need data about the jobs where I did get internships and where I didn't

Hopefully six months after graduation, I am in grad school, or at least admitted to one. I hope to pursue a masters in statistics. And then five years after graduation, I hope to be in some sort of statistics adjacent job, although I can't pin down what specific role that would be.

I hope to retire early.

I would say that my main interest is the underlying mathematics and theory with statistical methods. I hope to see more of that compared to regression.

### Team Applied Section

Find the misclassification rate for K-nearest neighbors (KNN) for a range of k values, given a complicated true generating model. Do this individually. As a team, compare answers and then plot the decision boundary for the team's "best" value of k.

Also, individually practice walking through the steps of Q1, Q2, and Q3 for this "project". Within this section will be a team section (what is the "best" k and what is the decision boundary) and individual sections. In your individual section, describe a plausible story for Q1, Q2, and Q3. Specifically, for Q1: What is Y, X1, and X2, and why should we care?. Make up something plausible that you actually care about. Y could be whether a kitten is adopted from a shelter given X1=age and X2=health of the cat. That's just one of countless plausible examples.

For Q2: Make predictions given X1 and X2 using KNN and k=(1 and 5) or (2 and 6) or (3 and 7) or (4 and 8). That is, each teammate fits KNN for two different values of k. Compute the misclassification rates for your values of k.

For Q3: Using this model and your plausible Q1 scenario, what is the answer for X1=0.5 and X2=0.5? For your scenario, what are some ethical implications of your stat learning modeling?

#### Generating Model

We have a logistic regression generating model. Given $x_1 \in [0,1]$ and $x_2 \in [0,1], Y \sim Ber(p)$, where $p$ is related to $x_1$ and $x_2$ through the logistic link function: $\log(\frac{p}{(1-p)}) = x_1 -2x_2 -2{x_1}^{2} + {x_2}^{2} + 3{x_1}{x_2}$, where $\log$ is the natural log, sometimes written as $\ln$.

The code for this is below.

```{r}
library(class)
library(tidyverse)
```

```{r}
#Generative model
set.seed(116) #setting a random seed so that we can reproduce everything exactly if we want to

generate_y <- function(x1,x2) { #two input parameters to generate the output y
  logit <- x1 -2*x2 -2*x1^2 + x2^2 + 3*x1*x2
  p <- exp(logit)/(1+exp(logit)) #apply the inverse logit function
  y <- rbinom(1,1,p) #y becomes a 0 (with prob 1-p) or a 1 with probability p
}
```

#### Example code

We are going to use our generating model to create a test set of 100 predictors (x1, x2), and then 100 outcomes. Then we plot all three variables just to see what the generating model is doing.

```{r}
# Generate a dataset with 100 points
set.seed(116)
n = 100
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 100 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #How many 0's and 1's were predicted? In this training set, 42% were 1's. However, almost 48% are 1's when n is large. That's quite close to 50/50 so we shouldn't have issues with "imbalance," which is something we'll learn about later in the semester.

training <- cbind(X1,X2,Y) #combining all of my variables into a training dataset
ggplot(data=training, aes(x=X1, y=X2, color=Y)) +
  geom_point()
```

This generating function seems to produce Y's with some spatial pattern in the X1, X2 parameter space, but the regions of 0's and 1's are not very well separated. How well will KNN do to classify/predict Y given new x1 and x2 values?

#### Test dataset

We are going to generate a new set of 100 predictors (x1, x2) and outcomes (y) that we will use as our "ground truth".

So, create the test dataset (using random seed=121) first.

```{r}
# Create the training dataset as above using seed=116
# Create a testing dataset using seed=121

set.seed(121)
n = 100
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 100 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #43 1's, which is much closer to the 51.5% true rate
testing <- cbind(X1,X2,Y)

#Let's plot the test set. Does it look like the training set? Yeah, looks similar.
ggplot(data=testing, aes(x=X1, y=X2, color=Y)) +
  geom_point()
```

#### What individuals need to do

1.  Given the training set (seed=116) and the testing set (seed=121), fit KNN on two different values of k.
2.  Calculate the misclassification rate for each k. If you don't know how to do this, ask a teammate or the professor.
3.  If possible, plot the decision boundaries for your k values.
4.  Summarize the Q1, Q2, and Q3 aspects of this "project." Use your imagination. Everyone should have a different scenario.
5.  Think about and discuss with your team some of the bonus questions below.

### Corva Individual Section 

```{r}
#fitting KNN on k=4

predicted.ys_4 <- knn(train = training[,1:2], 
                      test = testing[,1:2], 
                      cl=training[,3],
                      k=4)

#fitting KNN on k=8

predicted.ys_8 <- knn(train=training[,1:2],
                      test=testing[,1:2],
                      cl=training[,3],
                      k=8)
```
```{r}
#misclassification rate is number of incorrect predictions/ total number of predictions

#misclass rate for k=4
misclass_rate_4 = 1-(sum(predicted.ys_4==testing[,3])/length(predicted.ys_4))
misclass_rate_4

#misclass rate for k=8
misclass_rate_8 = 1-(sum(predicted.ys_8==testing[,3])/length(predicted.ys_8))
misclass_rate_8
```

Using the testing dataset as the truth, the misclassification rate for k=4 was 0.50. The misclassification rate for a higher K at k=8 was an improvement at 0.41. 

```{r}
#plot decision boundaries for k=4

# setting the grid that we will predict on 
testing.grid_4 <- expand.grid(X1 = seq(0, 1, by = .01), 
                            X2 = seq(0, 1, by = .01))
# predicting on the grid
predicted.grid_4 <- knn(train = training[,1:2], 
                      test = testing.grid_2, 
                      cl = training[,3], 
                      k = 4)

predicted.gridxy_4 <- testing.grid_4
predicted.gridxy_4$Y <- as.numeric(predicted.grid_8) -1

ggplot() +
  geom_raster(data = predicted.gridxy_4, aes(x = X1, y = X2, fill = as.factor(Y)), alpha = 0.3) +
  geom_contour(data = predicted.gridxy_4, aes(x = X1, y = X2, z = Y), 
               breaks = 1.5, color = "black", size = 1) +
  geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
  scale_fill_manual(values = c("orange", "cornflowerblue"), name = "Region") +
  scale_color_manual(values = c("darkorange", "blue"), name = "Actual Class") +
  labs(title = "KNN Decision Boundary (k=4)",
       subtitle = "Black line indicates the Bayes-style decision boundary") +
   geom_contour(data = predicted.gridxy_4, aes(x = X1, y = X2, z = Y +1),  
               breaks = 1.5, color = "black", size = 1) +
theme_minimal()



#plot decision boundaries for k=8

# setting the grid that we will predict on 
testing.grid_8 <- expand.grid(X1 = seq(0, 1, by = .01), 
                            X2 = seq(0, 1, by = .01))
# predicting on the grid
predicted.grid_8 <- knn(train = training[,1:2], 
                      test = testing.grid_2, 
                      cl = training[,3], 
                      k = 8)

predicted.gridxy_8 <- testing.grid_8
predicted.gridxy_8$Y <- as.numeric(predicted.grid_8) -1

ggplot() +
  geom_raster(data = predicted.gridxy_8, aes(x = X1, y = X2, fill = as.factor(Y)), alpha = 0.3) +
  geom_contour(data = predicted.gridxy_8, aes(x = X1, y = X2, z = Y), 
               breaks = 1.5, color = "black", size = 1) +
  geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
  scale_fill_manual(values = c("orange", "cornflowerblue"), name = "Region") +
  scale_color_manual(values = c("darkorange", "blue"), name = "Actual Class") +
  labs(title = "KNN Decision Boundary (k=8)",
       subtitle = "Black line indicates the Bayes-style decision boundary") +
   geom_contour(data = predicted.gridxy_8, aes(x = X1, y = X2, z = Y +1),  
               breaks = 1.5, color = "black", size = 1) +
theme_minimal()
```

```{r}
#observation of x1=0.5, x2=0.5
observation <- data.frame(X1 = 0.5, X2 = 0.5)

training_data_frame <- as.data.frame(training)

#Perform the KNN prediction
predicted_y <- knn(
  train = training[, c("X1", "X2")],       
  test = observation[, c("X1", "X2")],  
  cl = training_data_frame$Y,                        
  k = 8                                       
)
predicted_y
```

Q1: In this "project", Y was whether or not a borrower defaulted on their loan (default=1, no default=0), given their age(X1) and income (X2). This project is important for banks to understand which borrowers are risky to give loans to. 

Q2: The misclassification rate for k=4 was 0.5. We were able to correctly classify a borrower as defaulting or not only half of the time. The misclassification rate for a higher K at k=8 was an improvement at 0.41. With K=8, 41% of our predictions about borrowers paying their loans were incorrect. 

Q3: If the borrower was 50 years old (X1=0.5) and had a yearly income of $50,000 (X2=0.5), our model with K=8 predicts that they will not default on their loan. This loan modeling could have ethical implications since it only considers age and income, and allows no room for nuance. For example, retired people who might have smaller, stable income but no liabilities (i.e. their house and car are paid off) might be denied a loan whereas someone with high income but high liabilities might be approved for a loan.

### Parker Individual Section

```{r}
pred2k <- knn(train=training[,1:2], test=testing[,1:2], cl=training[,3], k=2)
pred6k <- knn(train=training[,1:2], test=testing[,1:2], cl=training[,3], k=10)

#Calculate Miscalculation rates for k=2 and k=6
truth <- testing[[3]]
pred2kn <- as.numeric(pred2k)
pred6kn <- as.numeric(pred6k)

misclass_rate <- function(predictions, truth) {
  1-(sum(predictions == truth)/length(predictions))
}
    
misclass_rate(pred2kn, testing[,3])
misclass_rate(pred6kn, testing[,3])
```
It seems that k=6 yielded a better result with a misclassification rate of 0.66 over the 0.69 misclassification rate with k=2.

```{r}
#Plot Decision Boundary for k=2 

testing.grid<-expand.grid(X1 =seq(0,1,by= .01),
X2 =seq(0,1,by= .01))

predicted.grid<-knn(train= training[,1:2],
test= testing.grid,
cl= training[,3],
k= 2)

predicted.gridxy<-testing.grid
predicted.gridxy$Y <-as.numeric(predicted.grid)-1

ggplot() + 
  geom_raster(data = predicted.gridxy, aes(x = X1, y = X2, fill = as.factor(Y)), alpha = 0.3) +
  geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y),
breaks = 1.5, color = "black", size = 1) +
  geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
scale_fill_manual(values = c("green", "red"), name = "Region") +
scale_color_manual(values = c("darkgreen", "darkred"), name = "Actual Class") +
labs(title = "KNN Decision Boundary (k=2)",
subtitle = "Black line indicates the Bayes-style decision boundary") +
geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y +1),
breaks = 1.5, color = "black", size = 1) + 
theme_minimal()

#Plot Decision Boudary for k=10

predicted.grid10<-knn(train= training[,1:2],
test= testing.grid,
cl= training[,3],
k= 10)

predicted.gridxy<-testing.grid
predicted.gridxy$Y <-as.numeric(predicted.grid10)-1

ggplot() + 
  geom_raster(data = predicted.gridxy, aes(x = X1, y = X2, fill = as.factor(Y)), alpha = 0.3) +
  geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y),
breaks = 1.5, color = "black", size = 1) +
  geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
scale_fill_manual(values = c("green", "red"), name = "Region") +
scale_color_manual(values = c("darkgreen", "darkred"), name = "Actual Class") +
labs(title = "KNN Decision Boundary (k=10)",
subtitle = "Black line indicates the Bayes-style decision boundary") +
geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y +1),
breaks = 1.5, color = "black", size = 1) + 
theme_minimal()
```
Q1: In this step we would try to understand why this is important or what types questions we can answer. A bank may ask how accurately they can predict fraud in their system given data X1 and X2. We would talk to domain experts to learn why this question is important: banks can better protect their customer's financial assets and also reduce financial losses to the bank itself. This is also the step where we would discuss data collection and the types of statistical learning methods that would be appropriate for the question at hand.

Q2: This step is where the statistical method we decided upon will be implemented and tested. In our case, we would be using the home address and the transaction amount data to test and train the KNN model. 

Q3: At this step we aim use the results obtained from the model and to understand what they mean. We may also ask how we can most effectively portray our results to non-domain experts. We will also discuss the limitations of the model.

#### What teams need to do

1.  Find the best k. Plot the decision boundary for that k.
2.  Answer as many of the bonus questions as you can.

### Team Section

```{r}
#Find the best k:
#Set seed since knn function breaks ties randomly
set.seed(123)
vals <- rep(0,50)

for(i in 1:50){
 preds <- knn(train=training[,1:2], test=testing[,1:2], cl=training[,3], k=i) 
 vals[i] <- misclass_rate(preds, testing[[3]])
}
```
It seems that the best k is 5 which has a miclassification rate of 0.38

```{r}
#Plot Decision Boundary for k=5

predicted.grid10<-knn(train= training[,1:2],
test= testing.grid,
cl= training[,3],
k= 5)

predicted.gridxy<-testing.grid
predicted.gridxy$Y <-as.numeric(predicted.grid10)-1

ggplot() + 
  geom_raster(data = predicted.gridxy, aes(x = X1, y = X2, fill = as.factor(Y)), alpha = 0.3) +
  geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y),
breaks = 1.5, color = "black", size = 1) +
  geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
scale_fill_manual(values = c("green", "red"), name = "Region") +
scale_color_manual(values = c("darkgreen", "darkred"), name = "Actual Class") +
labs(title = "KNN Decision Boundary (k=5)",
subtitle = "Black line indicates the Bayes-style decision boundary") +
geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y +1),
breaks = 1.5, color = "black", size = 1) + 
theme_minimal()
```
#### Bonus questions

Ultimately, we would like to know more about this problem, but we don't necessarily have all the tools yet to answer all of our questions. Here are some bonus questions:

-   What is the misclassification rate for k=1...2j, where j is the "optimal" k. That is, for a whole range of k values?
-   What is the Bayes decision boundary for the optimal k?
-   How is the misclassification rate broken down into variance, bias, and irreducible error of the KNN estimator? Related to these questions are how the misclassification rate changes when we use different training or test sets.
-   What happens to the misclassification rate if we go outside the $[0,1]^2$ parameter space? I.e., if $x_1$ and $x_2$ are $<0$ or $>1$?

**Note: the intended audience for your team document is your teammates, the professor, and your future self.**

**Some intended outcomes from this assignment:**

-   You and your team will learn how to effectively edit a document collaboratively
-   You will think about what you want to accomplish in life and how this course relates to that
-   Your teammates, the professor, and the TA will learn something more about you
-   You will get more experience applying statistical learning methods
-   You will gain experience collaborating with your teammates on an applied problem
-   Get practice evaluating a method, specifically KNN
-   Practice thinking about the whole problem (i.e., Q1Q2Q3, not just the Q2 quantitative parts)
